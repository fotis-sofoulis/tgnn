{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Preprocessing"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ea3637de5d3a30be"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Setup for Local User or Standalone Spark"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "6ce872ea68af7925"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# !apt-get install openjdk-8-jdk-headless -qq > /dev/null\n",
    "# !wget -q http://archive.apache.org/dist/spark/spark-3.4.1/spark-3.4.1-bin-hadoop3.tgz\n",
    "# !tar xf spark-3.4.1-bin-hadoop3.tgz\n",
    "# !pip install -q findspark\n",
    "# \n",
    "# import os\n",
    "# os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
    "# os.environ[\"SPARK_HOME\"] = \"/content/spark-3.4.1-bin-hadoop3\"\n",
    "# \n",
    "# import findspark\n",
    "# findspark.init()\n",
    "# from pyspark.sql import SparkSession\n",
    "# spark = SparkSession.builder.master(\"local[*]\").getOrCreate()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "21c6bef77df9cae8"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Setup for Spark Client via Kubernetes Cluster"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "bd8970fbd51fcc9f"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import socket\n",
    "from pyspark import SparkConf, SparkSession\n",
    "from pyspark.sql.functions import struct\n",
    "from neo4j import GraphDatabase\n",
    "\n",
    "sparkConf = SparkConf()\n",
    "\n",
    "host = socket.gethostname()\n",
    "hostname = socket.gethostbyname(host)\n",
    "\n",
    "print(f\"using host {host} and hostname {hostname}\")\n",
    "\n",
    "sparkConf.setMaster(\"k8s://https://kubernetes.default.svc:443\")\n",
    "sparkConf.setAppName(\"Preprocessing\")\n",
    "sparkConf.set(\"spark.kubernetes.namespace\", \"tgnnapp\")\n",
    "sparkConf.set(\"spark.kubernetes.authenticate.driver.serviceAccountName\", \"app-tgnnapp-spark\")\n",
    "sparkConf.set(\"spark.kubernetes.authenticate.executor.serviceAccountName\", \"app-tgnnapp-spark\")\n",
    "sparkConf.set(\"spark.kubernetes.authenticate.submission.caCertFile\", \"/var/run/secrets/kubernetes.io/serviceaccount/ca.crt\")\n",
    "sparkConf.set(\"spark.kubernetes.authenticate.submission.oauthTokenFile\", \"/var/run/secrets/kubernetes.io/serviceaccount/token\")\n",
    "sparkConf.set(\"spark.kubernetes.driver.pod.name\", host)\n",
    "sparkConf.set(\"spark.executor.instances\", \"2\")\n",
    "sparkConf.set(\"spark.kubernetes.container.image\", \"fotisofo/tgnn-spark:3.4.1-debian-11-r4--rev1\")\n",
    "sparkConf.set(\"spark.submit.deployMode\", \"client\")\n",
    "sparkConf.set(\"spark.driver.host\", hostname)\n",
    "\n",
    "sc = SparkSession.builder.config(conf=sparkConf).getOrCreate()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "initial_id"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Fetch Data"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "a2e5a4d39144ccc7"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# ==========Fetch data with neo4j==========\n",
    "\n",
    "# Define the Neo4j server connection details\n",
    "uri = \"bolt://app-tgnnapp-neo4j:7687\"\n",
    "user = \"<username>\"\n",
    "password = \"<password>\"\n",
    "\n",
    "# Define the query\n",
    "query = \"MATCH (u:User)-[r:INTERACTED]->(v:User) RETURN u.id AS user_u, v.id AS user_v, r.timestamp AS timestamp\"\n",
    "\n",
    "# Create driver and fetch the results\n",
    "driver = GraphDatabase.driver(uri, auth=(user, password))\n",
    "with driver.session() as session:\n",
    "    result = session.run(query)\n",
    "    data = [{'user_u': record['user_u'], 'user_v': record['user_v'], 'timestamp': record['timestamp']} for record in result]\n",
    "\n",
    "# Create dataframe from fetched data\n",
    "df = sc.createDataFrame(data).select(struct(\"*\").alias(\"p\"))\n",
    "\n",
    "# ==========Or Fetch data directly from csv file==========\n",
    "import pandas as pd\n",
    "url = '<csv_file>'\n",
    "df = sc.read.options(header='true', inferSchema='true', delimiter=',').csv(url)\n",
    "\n",
    "df.printSchema()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ac624bd9623ea13f"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Data Cleaning"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "511aa5022f893d6f"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df = df.dropna(subset=[\"user_u\", \"user_v\", \"timestamp\"])\n",
    "print(f\"Number of rows after removing missing values: {df.count()}\")"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "3d94c2624dc8d8d9"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df = df.dropDuplicates()\n",
    "print(f\"Number of rows after removing duplicate values: {df.count()}\")"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "a3b99edd3c645d23"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df = df.filter(df[\"timestamp\"] >= 0)\n",
    "print(f\"Number of rows after removing erroneous timestamps: {df.count()}\")"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "2bc154a5988109c5"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df = df.sort(\"timestamp\")"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "7b18182034c0a4b7"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Group timestamps into N-hour timesteps"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ccb39daf4f055634"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "\n",
    "min_timestamp = df.agg(F.min(\"timestamp\")).collect()[0][0]\n",
    "\n",
    "# Group the timestamp column into 6-hour(or N-hour) timesteps\n",
    "df_with_timesteps = df.withColumn(\"timestep\", ((F.col(\"timestamp\") - min_timestamp) / 21600).cast(\"integer\"))\n",
    "\n",
    "# Get the number of timesteps generated\n",
    "max_timestep = df_with_timesteps.agg(F.max(\"timestep\")).collect()[0][0]\n",
    "print(\"Maximum value in timestep column:\", max_timestep)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "6a070d1ce8f76540"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Additional grouping"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "5721886ab557b6ab"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# For the college dataset we will group further timesteps with 1 interaction into larger chunks\n",
    "from pyspark.sql import Window\n",
    "\n",
    "# First, compute the interactions per timestep\n",
    "interactions_per_timestep = df_with_timesteps.groupBy(\"timestep\").agg(F.count(\"*\").alias(\"num_interactions\"))\n",
    "\n",
    "# Compute the mean interactions\n",
    "mean_interactions = interactions_per_timestep.agg(F.avg(\"num_interactions\")).collect()[0][0]\n",
    "\n",
    "# Join the interactions count back to the original DataFrame\n",
    "df_with_timesteps = df_with_timesteps.join(interactions_per_timestep, on=\"timestep\", how=\"left\")\n",
    "\n",
    "# Create the cumulative sum column and bucket IDs\n",
    "windowSpec = Window.orderBy(\"timestep\")\n",
    "df_with_cumsum = df_with_timesteps.withColumn(\"cumulative_sum\", F.sum(\"num_interactions\").over(windowSpec))\n",
    "df_with_buckets = df_with_cumsum.withColumn(\"bucket\", (F.col(\"cumulative_sum\") / mean_interactions).cast(\"int\"))\n",
    "\n",
    "# Define the window specification\n",
    "bucketWindowSpec = Window.orderBy(\"bucket\").rowsBetween(Window.unboundedPreceding, Window.currentRow)\n",
    "\n",
    "# Create the 'bucket_id' column\n",
    "df_with_buckets = df_with_buckets.withColumn(\"bucket_id\", F.dense_rank().over(bucketWindowSpec))\n",
    "print(df_with_buckets.select('bucket').distinct().count())\n",
    "\n",
    "# Filter down unwanted columns\n",
    "df_with_buckets = df_with_buckets[['user_u', 'user_v', 'timestamp',  'timestep', 'bucket_id']]"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "bceff0c0f8c0ca23"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Create Affinity Matrix Visualisation"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "739e89808c1bddc4"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, sum\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Get number of bucket_ids into a list\n",
    "timesteps_list = df_with_buckets.select(\"bucket_id\").distinct().rdd.flatMap(lambda x: x).collect()\n",
    "\n",
    "# Create an empty list to hold dataframes for each timestep\n",
    "matrices = []\n",
    "\n",
    "for ts in range(2,6): # get 4 matrices\n",
    "    subset = df_with_buckets.filter(col(\"bucket_id\") == ts)\n",
    "\n",
    "    # Pivot dataframe and sum num_interactions to populate matrix\n",
    "    pivot_df = subset.groupBy(\"user_u\").pivot(\"user_v\").agg(sum(\"num_interactions\")).fillna(0)\n",
    "\n",
    "    matrices.append(pivot_df)\n",
    "\n",
    "for i, matrix in enumerate(matrices[:5]):\n",
    "    # Convert to Pandas DataFrame\n",
    "    pd_matrix = matrix.toPandas().set_index('user_u')\n",
    "\n",
    "    # Plotting\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    sns.heatmap(pd_matrix, cmap='rocket', linewidths=0.5)\n",
    "    plt.title(f\"Affinity Matrix for Timestep {i+2}\")\n",
    "    plt.show()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "4a53f9d6244463b"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Affinity Matrix Function"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "6c76e1ce31ab3daf"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from pyspark.sql import DataFrame\n",
    "\n",
    "# Create affinity matrix function\n",
    "def create_affinity_matrix(bucket_df: DataFrame) -> DataFrame:\n",
    "    # Count interactions per pair\n",
    "    interactions_per_pair = bucket_df.groupBy('user_u', 'user_v').agg(F.count(\"*\").alias('interactions_count'))\n",
    "\n",
    "    # Calculate the total interactions for proportional scaling\n",
    "    total_interactions = interactions_per_pair.groupBy().sum(\"interactions_count\").collect()[0][0]\n",
    "\n",
    "    # Apply proportional scaling\n",
    "    interactions_scaled = interactions_per_pair.withColumn(\"scaled_interactions\", F.col(\"interactions_count\") / total_interactions)\n",
    "\n",
    "    return interactions_scaled\n",
    "\n",
    "bucket_ids = df_with_buckets.select(\"bucket_id\").distinct().rdd.flatMap(lambda x: x).collect()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "f654920797fdefc6"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Total Modulation Function"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "3061e3d4cd16fc71"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def modularity(graph_df, cluster_df):\n",
    "    \"\"\"\n",
    "    Compute modularity of a directed graph after clustering.\n",
    "\n",
    "    graph_df: DataFrame with columns ['src', 'dst', 'weight']\n",
    "    cluster_df: DataFrame with columns ['id', 'cluster']\n",
    "\n",
    "    Returns: Modularity score\n",
    "    \"\"\"\n",
    "    m = graph_df.agg({\"weight\": \"sum\"}).collect()[0][0]\n",
    "\n",
    "    graph_df = graph_df.withColumn(\"src\", col(\"src\").cast(\"long\"))\n",
    "    graph_df = graph_df.withColumn(\"dst\", col(\"dst\").cast(\"long\"))\n",
    "\n",
    "    # Joining graph with cluster assignments\n",
    "    src_cluster_df = cluster_df.withColumnRenamed('id', 'src_id').withColumnRenamed('cluster', 'src_cluster')\n",
    "    dst_cluster_df = cluster_df.withColumnRenamed('id', 'dst_id').withColumnRenamed('cluster', 'dst_cluster')\n",
    "\n",
    "    df = graph_df.join(src_cluster_df, col('src') == col('src_id'), 'inner')\n",
    "    df = df.join(dst_cluster_df, col('dst') == col('dst_id'), 'inner')\n",
    "\n",
    "    # Computing delta function for pairs in the same cluster\n",
    "    df = df.withColumn('delta', (col('src_cluster') == col('dst_cluster')).cast('int'))\n",
    "\n",
    "    # Computing the out-degree and in-degree products\n",
    "    out_degree = (graph_df.groupBy('src').agg({\"weight\": \"sum\"}).withColumnRenamed('src', 'node').withColumnRenamed('sum(weight)', 'out_weight'))\n",
    "    in_degree = (graph_df.groupBy('dst').agg({\"weight\": \"sum\"}).withColumnRenamed('dst', 'node').withColumnRenamed('sum(weight)', 'in_weight'))\n",
    "\n",
    "   # Outer join to keep all nodes\n",
    "    degrees = out_degree.join(in_degree, 'node', 'outer')\n",
    "\n",
    "    # Handle potential null values\n",
    "    degrees = degrees.fillna(0, subset=['out_weight', 'in_weight'])\n",
    "    degrees = degrees.withColumn('degree_product', col('out_weight') * col('in_weight'))\n",
    "\n",
    "    df = df.join(degrees.withColumnRenamed('node', 'src'), 'src', 'inner')\n",
    "\n",
    "    # Computing the modularity components for each edge\n",
    "    df = df.withColumn('modularity', (col('weight') - col('degree_product') / m) * col('delta'))\n",
    "\n",
    "    # Summing over all edges to get the total modularity\n",
    "    modularity_sum = df.agg({\"modularity\": \"sum\"}).collect()[0][0]\n",
    "    total_modularity = modularity_sum / m if modularity_sum is not None else 0\n",
    "\n",
    "    return total_modularity"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "f5e8d0c44a47088d"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Generate Features, Clustering and Modularity Calculation"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "22b39d576fe762da"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from pyspark.ml.clustering import PowerIterationClustering\n",
    "from pyspark.sql import Row\n",
    "\n",
    "cumulative_data = None\n",
    "max_cumulative_buckets = 3\n",
    "interactions_list = []\n",
    "modularities = []\n",
    "\n",
    "bucket_ids_iter = sorted(bucket_ids)\n",
    "\n",
    "for bucket in bucket_ids_iter:\n",
    "    # Create a cumulative df of max_cumulative_buckets\n",
    "    lower_bound = bucket - max_cumulative_buckets + 1\n",
    "    cumulative_data = df_with_buckets.filter((F.col(\"bucket_id\") >= lower_bound) & (F.col(\"bucket_id\") <= bucket))\n",
    "    \n",
    "    print(cumulative_data.select(\"bucket_id\").distinct().collect())\n",
    "    \n",
    "    # Create the affinity matrix\n",
    "    affinity_df = create_affinity_matrix(cumulative_data)\n",
    "    adjusted_df = affinity_df.withColumnRenamed(\"user_u\", \"src\").withColumnRenamed(\"user_v\", \"dst\").withColumnRenamed(\"scaled_interactions\", \"weight\")\n",
    "    \n",
    "    # Apply Power Iteration Clustering\n",
    "    pic = PowerIterationClustering(k=3, maxIter=20, initMode=\"degree\", weightCol=\"weight\")\n",
    "    \n",
    "    assignments  = pic.assignClusters(adjusted_df)\n",
    "    \n",
    "    # Get modularity of the clustered data for the first 100 samples\n",
    "    if (bucket <= 100):\n",
    "        # Get modularity for \n",
    "        current_modularity = modularity(adjusted_df, assignments)  \n",
    "        modularities.append(current_modularity)\n",
    "    \n",
    "    interactions_with_clusters = adjusted_df.join(assignments.withColumnRenamed(\"id\", \"src\").withColumnRenamed(\"cluster\", \"src_cluster\"), on=\"src\") \\\n",
    "                                        .join(assignments.withColumnRenamed(\"id\", \"dst\").withColumnRenamed(\"cluster\", \"dst_cluster\"), on=\"dst\") \\\n",
    "                                        .select(\"src\", \"src_cluster\", \"dst\", \"dst_cluster\")\n",
    "    \n",
    "    \n",
    "    # Convert the interactions with clusters to a list and append to the master list\n",
    "    current_interactions = interactions_with_clusters.rdd.map(lambda row: Row(src_id=row.src, src_cluster=row.src_cluster, dst_id=row.dst, dst_cluster=row.dst_cluster, bucket_id=bucket)).collect()\n",
    "    interactions_list.extend(current_interactions)\n",
    "\n",
    "\n",
    "# Convert the list to DataFrame\n",
    "interactions_df = sc.createDataFrame(interactions_list)\n",
    "interactions_df.show()\n",
    "# Save DataFrame to CSV\n",
    "# interactions_df.write.csv(\"/data/CollegeMsgClusters\", header=True) # multiple nodes\n",
    "interactions_df.coalesce(1).write.csv(\"/data/CollegeMsgClustersFull\", header=True) # single node"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "396a971f7fe525f5"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Visualise Modularity"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "f14bfecf0c9c3e04"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Plotting\n",
    "mod_list = sorted(bucket_ids)[:100]\n",
    "plt.plot(mod_list, modularities, marker='o')\n",
    "plt.xlabel('Bucket ID')\n",
    "plt.ylabel('Modularity')\n",
    "plt.title('Modularity over each bucket')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "38665bcc89946239"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
